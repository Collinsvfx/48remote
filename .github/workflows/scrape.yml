name: Scrape Jobs Every 20 Minutes

on:
  schedule:
    # Runs every 20 minutes (at 0, 20, 40 past the hour)
    - cron: '*/20 * * * *'
  workflow_dispatch: # Allows manual trigger

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 25  # avoid stuck runs

    steps:
      - name: üß≠ Checkout code
        uses: actions/checkout@v4

      - name: üêç Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: üß∞ Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install playwright requests beautifulsoup4 lxml
          playwright install chromium --with-deps

      - name: üß† Run scraper with safe flags
        env:
          PYTHONUNBUFFERED: 1
          PLAYWRIGHT_BROWSERS_PATH: 0
        run: |
          python - <<'PYCODE'
          import subprocess
          print("--- Launching scraper (CI safe mode) ---")
          subprocess.run(
              ["python", "scraper.py"],
              check=True,
              env={
                  **dict(__import__("os").environ),
                  "PLAYWRIGHT_HEADLESS": "true",
                  "PLAYWRIGHT_LAUNCH_ARGS": "--no-sandbox --disable-setuid-sandbox --disable-gpu"
              }
          )
          PYCODE

      - name: üíæ Commit and push jobs.json (if changed)
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git add jobs.json
          git diff --quiet && git diff --staged --quiet || \
            (git commit -m "Update jobs.json [$(date)]" && git push)
